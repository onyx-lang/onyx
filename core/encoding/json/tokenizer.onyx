// Everything in this file is marked #package because I do not think
// that this code will be needed outside of this module. I do not see
// the value of having access to the tokenizer and parser of JSON directly.


package core.encoding.json
#allow_stale_code

#package
Tokenizer :: struct {
    data: [] u8
    use position := Position.{ 0, 1, 1 }
}

#package
Token :: struct {
    Kind :: enum {
        Invalid

        Open_Brace;    // {
        Close_Brace;   // }

        Open_Bracket;  // [
        Close_Bracket; // ]

        Comma
        Colon

        Null
        True
        False

        Integer
        Float
        String
    }

    kind: Kind = .Invalid
    text: str  = null_str
    use position := Position.{ 0, 1, 1 }
}

#package
Position :: struct {
    offset       : u32;  // Offset into the stream
    line, column : u32;  // Line and column number
}

#package
token_get :: (use tkn: &Tokenizer) -> (Token, Error) {
    err := Error.{}

    skip_whitespace(tkn)
    token := Token.{}
    token.position = tkn.position

    curr_char := data[offset]
    next_char, has_next := next_character(tkn)
    if !has_next do return .{}, .{ .EOF, token.position }

    switch curr_char {
        case '{' do token.kind = .Open_Brace
        case '}' do token.kind = .Close_Brace
        case '[' do token.kind = .Open_Bracket
        case ']' do token.kind = .Close_Bracket
        case ',' do token.kind = .Comma
        case ':' do token.kind = .Colon

        case 'a' ..= 'z' {
            token.kind = .Invalid
            skip_alpha_numeric(tkn)

            identifier := data.data[token.offset .. offset]
            if identifier == "null"  do token.kind = .Null
            if identifier == "true"  do token.kind = .True
            if identifier == "false" do token.kind = .False
        }

        case '-' {
            switch data[offset] {
                case '0' ..= '9' ---
                case _ {
                    err.kind = .Illegal_Character
                    err.pos  = token.position
                    break break
                }
            }

            fallthrough
        }

        case '0' ..= '9' {
            token.kind = .Integer
            skip_numeric(tkn)

            if data[offset] == '.' {
                token.kind = .Float
                next_character(tkn)
                skip_numeric(tkn)
            }

            if data[offset] == 'e' || data[offset] == 'E' {
                next_character(tkn)
                if data[offset] == '-' || data[offset] == '+' {
                    next_character(tkn)
                }
                skip_numeric(tkn)
            }
        }

        case '"' {
            token.kind = .String

            while offset < data.count {
                ch := data[offset]
                if ch == '\n' {
                    err.kind = .String_Unterminated
                    err.pos  = token.position
                    break break
                }

                next_character(tkn)
                if ch == '"' {
                    break
                }

                if ch == '\\' {
                    skip_escape(tkn)
                }
            }
        }
    }

    token.text = data.data[token.offset .. offset]

    if token.kind == .Invalid do err.kind = .Illegal_Character

    return token, err
}

#local
next_character :: (use tkn: &Tokenizer) -> (u8, bool) {
    if offset >= data.count do return 0, false

    retval := data[offset]
    offset += 1
    column += 1

    return retval, true
}

#local
skip_whitespace :: (use tkn: &Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case '\t', ' ', '\r', '\v' {
                next_character(tkn)
            }

            case '\n' {
                line += 1
                column = 1
                offset += 1
            }

            case _ {
                break break
            }
        }
    }
}

#local
skip_alpha_numeric :: (use tkn: &Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case 'A' ..= 'Z', 'a' ..= 'z', '0' ..= '9', '_' {
                next_character(tkn)
                continue
            }
        }

        break
    }
}

#local
skip_numeric :: (use tkn: &Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case '0' ..= '9' {
                next_character(tkn)
                continue
            }
        }

        break
    }
}

#local
skip_escape :: (use tkn: &Tokenizer) {
    switch data[offset] {
        case 'u' {
            for i in 4 {
                ch, _ := next_character(tkn)
                switch ch {
                    case '0' ..= '9',
                         'A' ..= 'F',
                         'a' ..= 'f' ---

                    case _ do return
                }
            }
        }

        case _ {
            next_character(tkn)
        }
    }
}
