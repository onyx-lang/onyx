// Everything in this file is marked #package because I do not think
// that this code will be needed outside of this module. I do not see
// the value of having access to the tokenizer and parser of JSON directly.


package core.encoding.json
#allow_stale_code

#package
Tokenizer :: struct {
    data: [] u8;
    use position := Position.{ 0, 1, 1 };
}

#package
Token :: struct {
    Kind :: enum {
        Invalid;

        Open_Brace;    // {
        Close_Brace;   // }

        Open_Bracket;  // [
        Close_Bracket; // ]

        Comma;
        Colon;

        Null;
        True;
        False;

        Integer;
        Float;
        String;
    }

    kind: Kind = .Invalid;
    text: str  = null_str;
    use position := Position.{ 0, 1, 1 };
}

#package
Position :: struct {
    offset       : u32;  // Offset into the stream
    line, column : u32;  // Line and column number
}

#package
token_get :: (use tkn: ^Tokenizer) -> (Token, Error) {
    err := Error.{};

    skip_whitespace(tkn);
    token := Token.{};
    token.position = tkn.position;

    curr_char := data[offset];
    next_char, has_next := next_character(tkn);
    if !has_next do return .{}, .{ .EOF, token.position };

    switch curr_char {
        case '{' do token.kind = .Open_Brace;
        case '}' do token.kind = .Close_Brace;
        case '[' do token.kind = .Open_Bracket;
        case ']' do token.kind = .Close_Bracket;
        case ',' do token.kind = .Comma;
        case ':' do token.kind = .Colon;

        case 'a' ..= 'z' {
            token.kind = .Invalid;
            skip_alpha_numeric(tkn);

            identifier := data.data[token.offset .. offset];
            if identifier == "null"  do token.kind = .Null;
            if identifier == "true"  do token.kind = .True;
            if identifier == "false" do token.kind = .False;
        }

        case '-' {
            switch data[offset] {
                case '0' ..= '9' ---
                case _ {
                    err.kind = .Illegal_Character;
                    err.pos  = token.position;
                    break break;
                }
            }

            fallthrough;
        }

        case '0' ..= '9' {
            token.kind = .Integer;
            skip_numeric(tkn);

            if data[offset] == '.' {
                token.kind = .Float;
                next_character(tkn);
                skip_numeric(tkn);
            }

            if data[offset] == 'e' || data[offset] == 'E' {
                next_character(tkn);
                if data[offset] == '-' || data[offset] == '+' {
                    next_character(tkn);
                }
                skip_numeric(tkn);
            }
        }

        case '"' {
            token.kind = .String;

            while offset < data.count {
                ch := data[offset];
                if ch == '\n' {
                    err.kind = .String_Unterminated;
                    err.pos  = token.position;
                    break break;
                }

                next_character(tkn);
                if ch == '"' {
                    break;
                }

                if ch == '\\' {
                    skip_escape(tkn);
                }
            }
        }
    }

    token.text = data.data[token.offset .. offset];

    if token.kind == .Invalid do err.kind = .Illegal_Character;

    return token, err;
}

#local
next_character :: (use tkn: ^Tokenizer) -> (u8, bool) {
    if offset >= data.count do return 0, false;

    retval := data[offset];
    offset += 1;
    column += 1;

    return retval, true;
}

#local
skip_whitespace :: (use tkn: ^Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case '\t', ' ', '\r', '\v' {
                next_character(tkn);
            }

            case '\n' {
                line += 1;
                column = 1;
                offset += 1;
            }

            case _ {
                break break;
            }
        }
    }
}

#local
skip_alpha_numeric :: (use tkn: ^Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case 'A' ..= 'Z', 'a' ..= 'z', '0' ..= '9', '_' {
                next_character(tkn);
                continue;
            }
        }

        break;
    }
}

#local
skip_numeric :: (use tkn: ^Tokenizer) {
    while offset < data.count {
        switch data[offset] {
            case '0' ..= '9' {
                next_character(tkn);
                continue;
            }
        }

        break;
    }
}

#local
skip_escape :: (use tkn: ^Tokenizer) {
    switch data[offset] {
        case 'u' {
            for i in 4 {
                ch, _ := next_character(tkn);
                switch ch {
                    case '0' ..= '9',
                         'A' ..= 'F',
                         'a' ..= 'f' ---

                    case _ do return;
                }
            }
        }

        case _ {
            next_character(tkn);
        }
    }
}
