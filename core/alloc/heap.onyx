package core.alloc.heap

use runtime
use core


// This is the implementation for the general purpose heap allocator.
// It is a simple bump allocator, with a free list. It is not very good
// but it suffices for the kinds of things being done in the early days
// of the language. You will not make your own instance of the heap
// allocator, since it controls WASM intrinsics such as memory_grow.



// Enable this to enable checking for invalid blocks and other corruptions
// that may happen on the heap, with the added overhead of checking that
// on every alloc/resize/free.
#local Enable_Debug :: #defined( runtime.vars.Enable_Heap_Debug )
#local Enable_Clear_Freed_Memory :: #defined(runtime.vars.Enable_Heap_Clear_Freed_Memory)
#local Enable_Stack_Trace :: runtime.Stack_Trace_Enabled

#load "core:intrinsics/wasm"

#if runtime.Multi_Threading_Enabled {
    use core {sync}

    heap_mutex: sync.Mutex
}

init :: () {
    heap_state.free_list = null;
    heap_state.next_alloc = cast(rawptr) (cast(uintptr) __heap_start + 8);
    heap_state.remaining_space = (memory_size() << 16) - cast(u32) __heap_start;

    use core.alloc { heap_allocator }
    heap_allocator.data = &heap_state;
    heap_allocator.func = heap_alloc_proc;

    #if runtime.Multi_Threading_Enabled {
        sync.mutex_init(&heap_mutex);
    }
}

get_watermark  :: () => cast(u32) heap_state.next_alloc;
get_freed_size :: () => {
    total := 0;
    block := heap_state.free_list;
    while block != null {
        total += block.size;
        block = block.next;
    }
    return total;
}

// See the comment in onyx_library.h as to why these don't exist anymore.
//
// #if !#defined(runtime.vars.Dont_Export_Heap_Functions) {
//     // heap_alloc is not exported because you can use __heap_resize(NULL, size)
//     #export "__heap_resize" heap_resize
//     #export "__heap_free"   heap_free
// }

#local {
    use core.intrinsics.wasm {
        memory_size, memory_grow,
        memory_copy, memory_fill,
        memory_equal,
    }

    use core {memory, math}

    uintptr :: #type u32

    // The global heap state
    heap_state : struct {
        free_list       : &heap_freed_block;
        next_alloc      : rawptr;
        remaining_space : u32;
    }

    heap_block :: struct {
        size         : u32;
        magic_number : u32;
    }

    heap_freed_block :: struct {
        use base: heap_block;
        next : &heap_freed_block;
        prev : &heap_freed_block;
    }

    heap_allocated_block :: struct {
        use base: heap_block;
    }

    Allocated_Flag           :: 0x1
    Free_Block_Magic_Number  :: 0xdeadbeef
    Alloc_Block_Magic_Number :: 0xbabecafe
    Block_Split_Size         :: 256

    // FIX: This does not respect the choice of alignment
    heap_alloc :: (size_: u32, align: u32) -> rawptr {
        if size_ == 0 do return null;

        #if runtime.Multi_Threading_Enabled do sync.scoped_mutex(&heap_mutex);

        size := size_ + sizeof heap_block;
        size = math.max(size, sizeof heap_freed_block);
        memory.align(~~&size, ~~align);

        prev := &heap_state.free_list;
        hb := heap_state.free_list;

        best_extra := 0xffffffff;
        best: typeof hb = null;
        best_prev: typeof prev = null;

        while hb != null {
            if hb.size >= size {
                extra := hb.size - size;
                if extra < best_extra {
                    best = hb;
                    best_prev = prev;
                    best_extra = extra;
                }
            }

            prev = &hb.next;
            hb = hb.next;
        }

        if best != null {
            #if Enable_Debug {
                assert(best.size & Allocated_Flag == 0, "Allocated block in free list.");
                assert(best.magic_number == Free_Block_Magic_Number, "Malformed free block in free list.");
            }

            if best.size - size >= Block_Split_Size {
                new_block := cast(&heap_freed_block) (cast(uintptr) best + size);
                new_block.size = best.size - size;
                new_block.next = best.next;
                new_block.prev = best.prev;
                new_block.magic_number = Free_Block_Magic_Number;
                best.size = size;

                if best.next != null do best.next.prev = new_block;
                *best_prev = new_block;

            } else {
                if best.next != null do best.next.prev = best.prev;
                *best_prev = best.next;
            }

            best.next = null;
            best.prev = null;
            best.magic_number = 0;
            best.size |= Allocated_Flag;
            best.magic_number = Alloc_Block_Magic_Number;
            return cast(rawptr) (cast(uintptr) best + sizeof heap_allocated_block);
        }

        if size < heap_state.remaining_space {
            ret := cast(&heap_allocated_block) heap_state.next_alloc;
            ret.size = size;
            ret.size |= Allocated_Flag;
            ret.magic_number = Alloc_Block_Magic_Number;

            heap_state.next_alloc = cast(rawptr) (cast(uintptr) heap_state.next_alloc + size);
            heap_state.remaining_space -= size;

            return cast(rawptr) (cast(uintptr) ret + sizeof heap_allocated_block);
        }

        new_pages := ((size - heap_state.remaining_space) >> 16) + 1;
        if memory_grow(new_pages) == -1 {
            // out of memory
            return null;
        }
        heap_state.remaining_space += new_pages << 16;

        ret := cast(&heap_allocated_block) heap_state.next_alloc;
        ret.size = size;
        ret.size |= Allocated_Flag;
        ret.magic_number = Alloc_Block_Magic_Number;

        heap_state.next_alloc = cast(rawptr) (cast(uintptr) heap_state.next_alloc + size);
        heap_state.remaining_space -= size;

        return cast(rawptr) (cast(uintptr) ret + sizeof heap_allocated_block);
    }

    heap_free :: (ptr: rawptr) {
        #if Enable_Debug do assert(ptr != null, "Trying to free a null pointer.");

        hb_ptr := cast(&heap_freed_block) (cast(uintptr) ptr - sizeof heap_allocated_block);

        //
        // If this block was originally allocated from the GC space,
        // and then marked an "manually managed", we can free the block
        // as normal here, but we have to go back some more bytes.
        if hb_ptr.magic_number == core.alloc.gc.GC_Manually_Free_Magic_Number  {
            hb_ptr = ~~(cast(uintptr) (cast([&] core.alloc.gc.GCLink, ptr) - 1) - sizeof heap_allocated_block);
        }

        #if runtime.Multi_Threading_Enabled do sync.scoped_mutex(&heap_mutex);

        #if Enable_Debug {
            // RELOCATE
            trace :: macro () {
                #if Enable_Stack_Trace {
                    trace := runtime.info.get_stack_trace();
                    for trace {
                        log(.Error, "Core", core.tprintf("in {} ({}:{})", it.info.func_name, it.info.file, it.current_line));
                    }
                }
            }

            if cast(uintptr) hb_ptr < cast(uintptr) __heap_start {
                log(.Error, "Core", "FREEING STATIC DATA");
                trace();
                return;
            }

            if hb_ptr.size & Allocated_Flag != Allocated_Flag {
                log(.Error, "Core", "INVALID DOUBLE FREE");
                trace();
                return;
            }

            if hb_ptr.magic_number != Alloc_Block_Magic_Number {
                log(.Error, "Core", "FREEING INVALID BLOCK");
                trace();
                return;
            }

        } else {
            //
            // If not in debug mode, still catch the wierd cases and prevent them from breaking
            // the free list, as this will certainly cause terrible bugs that take hours to fix.
            //

            if cast(uintptr) hb_ptr < cast(uintptr) __heap_start {
                return;
            }

            if hb_ptr.size & Allocated_Flag != Allocated_Flag {
                return;
            }

            if hb_ptr.magic_number != Alloc_Block_Magic_Number {
                return;
            }
        }

        hb_ptr.size &= ~Allocated_Flag;
        orig_size := hb_ptr.size - sizeof heap_allocated_block;

        #if Enable_Debug && Enable_Clear_Freed_Memory {
            memory_fill(ptr, ~~0xcc, orig_size);
        }

        if cast(uintptr) hb_ptr + hb_ptr.size < cast(uintptr) heap_state.next_alloc {
            next_block := cast(&heap_freed_block) (cast(uintptr) hb_ptr + hb_ptr.size);

            if next_block.size & Allocated_Flag == 0 && next_block.magic_number == Free_Block_Magic_Number {
                hb_ptr.size += next_block.size;

                if next_block.next != null do next_block.next.prev = next_block.prev;
                if next_block.prev != null do next_block.prev.next = next_block.next;
                else                       do heap_state.free_list = next_block.next;

                next_block.next = null;
                next_block.prev = null;
            }
        }

        // This is an awful way to do this, BUT it works for now.
        // This looks for the block before the block being freed in order
        // to merge them into one large continuous block. This should just
        // be able to peek behind the block being freed and see if it is
        // another freed block, but in order to do that, a footer needs to
        // be placed every freed block. This sounds like too much work right
        // now as it will envitably require hours of debugging one incorrect
        // statement.                               - brendanfh 2022/01/16
        {
            walker := heap_state.free_list;
            while walker != null {
                after_block := cast(&heap_freed_block) (cast(uintptr) walker + walker.size);
                if after_block == hb_ptr {
                    hb_ptr.next = null;
                    hb_ptr.prev = null;

                    walker.size += hb_ptr.size;
                    return;
                }
                walker = walker.next;
            }
        }

        hb_ptr.magic_number = Free_Block_Magic_Number;
        hb_ptr.prev = null;
        hb_ptr.next = heap_state.free_list;

        if heap_state.free_list != null do heap_state.free_list.prev = hb_ptr;
        heap_state.free_list = hb_ptr;
    }

    heap_resize :: (ptr: rawptr, new_size_: u32, align: u32) -> rawptr {
        if ptr == null do return heap_alloc(new_size_, align);

        #if runtime.Multi_Threading_Enabled do sync.scoped_mutex(&heap_mutex);

        new_size := new_size_ + sizeof heap_block;
        new_size = math.max(new_size, sizeof heap_freed_block);
        new_size = ~~memory.align(cast(u64) new_size, ~~align);

        hb_ptr := cast(&heap_allocated_block) (cast(uintptr) ptr - sizeof heap_allocated_block);
        #if Enable_Debug do assert(hb_ptr.size & Allocated_Flag == Allocated_Flag, "Corrupted heap on resize.");
        hb_ptr.size &= ~Allocated_Flag;

        old_size := hb_ptr.size;

        // If there is already enough space in the current allocated block,
        // just return the block that already exists and has the memory in it.
        if old_size >= new_size {
            hb_ptr.size |= Allocated_Flag;
            return ptr;
        }

        // If we are at the end of the allocation space, just extend it
        if cast(uintptr) hb_ptr + hb_ptr.size >= cast(uintptr) heap_state.next_alloc {
            needed_size := cast(u32) memory.align(cast(u64) (new_size - old_size), 16);

            if needed_size >= heap_state.remaining_space {
                new_pages := ((needed_size - heap_state.remaining_space) >> 16) + 1;
                if memory_grow(new_pages) == -1 {
                    // out of memory
                    return null;
                }
                heap_state.remaining_space += new_pages << 16;
            }

            hb_ptr.size = new_size;
            hb_ptr.size |= Allocated_Flag;
            hb_ptr.magic_number = Alloc_Block_Magic_Number;
            heap_state.next_alloc = cast(rawptr) (cast(uintptr) heap_state.next_alloc + needed_size);
            heap_state.remaining_space -= needed_size;
            return ptr;
        }

        hb_ptr.size |= Allocated_Flag;
        new_ptr := heap_alloc(new_size_, align);
        #if runtime.Multi_Threading_Enabled do sync.mutex_lock(&heap_mutex);

        memory_copy(new_ptr, ptr, old_size - sizeof heap_block);
        heap_free(ptr);
        return new_ptr;
    }

    heap_alloc_proc :: (data: rawptr, aa: AllocationAction, size: u32, align: u32, oldptr: rawptr) -> rawptr {
        switch aa {
            case .Alloc  do return heap_alloc(size, align);
            case .Resize do return heap_resize(oldptr, size, align);
            case .Free   do heap_free(oldptr);
        }

        return null;
    }
}

